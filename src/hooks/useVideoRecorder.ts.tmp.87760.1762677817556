import { useState, useRef, useCallback } from 'react';

export interface VideoRecorderState {
  isRecording: boolean;
  recordedBlob: Blob | null;
  recordingDuration: number;
  error: string;
  isAutoGenerating: boolean;
  autoProgress: number;
}

export const useVideoRecorder = (audioElementRef?: React.RefObject<HTMLAudioElement>) => {
  const [isRecording, setIsRecording] = useState(false);
  const [recordedBlob, setRecordedBlob] = useState<Blob | null>(null);
  const [recordingDuration, setRecordingDuration] = useState(0);
  const [error, setError] = useState('');
  const [isAutoGenerating, setIsAutoGenerating] = useState(false);
  const [autoProgress, setAutoProgress] = useState(0);

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const startTimeRef = useRef<number>(0);
  const durationIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const destinationRef = useRef<MediaStreamAudioDestinationNode | null>(null);
  const sourceNodeRef = useRef<MediaElementAudioSourceNode | null>(null);

  const startRecording = useCallback(async () => {
    try {
      setError('');
      setRecordedBlob(null);
      chunksRef.current = [];

      // Get canvas element
      const canvas = document.querySelector('canvas');
      if (!canvas) {
        throw new Error('Canvas not found');
      }

      // Capture canvas stream
      const canvasStream = canvas.captureStream(30); // 30 FPS

      // Get audio element
      const audioElement = document.querySelector('audio') as HTMLAudioElement;
      let combinedStream: MediaStream;

      if (audioElement) {
        // Create AudioContext for audio capture (only once)
        if (!audioContextRef.current) {
          audioContextRef.current = new AudioContext();
          const source = audioContextRef.current.createMediaElementSource(audioElement);
          sourceNodeRef.current = source;

          // Keep connection to speakers
          source.connect(audioContextRef.current.destination);
        }

        // Create new destination for this recording
        const destination = audioContextRef.current.createMediaStreamDestination();

        // Connect existing source to new destination
        if (sourceNodeRef.current) {
          sourceNodeRef.current.connect(destination);
        }

        destinationRef.current = destination;

        // Combine canvas and audio
        combinedStream = new MediaStream([
          ...canvasStream.getVideoTracks(),
          ...destination.stream.getAudioTracks()
        ]);
      } else {
        // Just canvas (no audio)
        combinedStream = canvasStream;
      }

      // Create MediaRecorder
      const mimeType = MediaRecorder.isTypeSupported('video/webm;codecs=vp9')
        ? 'video/webm;codecs=vp9'
        : 'video/webm';

      const mediaRecorder = new MediaRecorder(combinedStream, {
        mimeType,
        videoBitsPerSecond: 5000000 // 5 Mbps
      });

      mediaRecorder.ondataavailable = (event) => {
        if (event.data && event.data.size > 0) {
          chunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = () => {
        const blob = new Blob(chunksRef.current, { type: mimeType });
        setRecordedBlob(blob);
        setIsRecording(false);

        if (durationIntervalRef.current) {
          clearInterval(durationIntervalRef.current);
        }
      };

      mediaRecorder.onerror = (event) => {
        console.error('MediaRecorder error:', event);
        setError('Recording error occurred');
        setIsRecording(false);
      };

      mediaRecorderRef.current = mediaRecorder;
      mediaRecorder.start(100); // Collect data every 100ms
      setIsRecording(true);

      // Track recording duration
      startTimeRef.current = Date.now();
      durationIntervalRef.current = setInterval(() => {
        const elapsed = Math.floor((Date.now() - startTimeRef.current) / 1000);
        setRecordingDuration(elapsed);
      }, 1000);

    } catch (err) {
      const errorMessage = err instanceof Error ? err.message : 'Failed to start recording';
      setError(errorMessage);
      console.error('Recording error:', err);
    }
  }, []);

  const stopRecording = useCallback(() => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();

      // Stop all tracks
      mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());

      if (durationIntervalRef.current) {
        clearInterval(durationIntervalRef.current);
      }
    }
  }, [isRecording]);

  const downloadRecording = useCallback(() => {
    if (!recordedBlob) return;

    const url = URL.createObjectURL(recordedBlob);
    const a = document.createElement('a');
    a.href = url;
    a.download = `neural-visuals-${Date.now()}.webm`;
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    URL.revokeObjectURL(url);
  }, [recordedBlob]);

  const clearRecording = useCallback(() => {
    setRecordedBlob(null);
    setRecordingDuration(0);
  }, []);

  // Auto-generate video from audio file
  const autoGenerateVideo = useCallback(async (duration: number) => {
    try {
      setError('');
      setRecordedBlob(null);
      setIsAutoGenerating(true);
      setAutoProgress(0);
      chunksRef.current = [];

      // Get canvas element
      const canvas = document.querySelector('canvas');
      if (!canvas) {
        throw new Error('Canvas not found');
      }

      // Get audio element
      const audioElement = document.querySelector('audio') as HTMLAudioElement;
      if (!audioElement) {
        throw new Error('Audio not loaded');
      }

      // Reset audio to beginning
      audioElement.currentTime = 0;

      // Capture canvas stream
      const canvasStream = canvas.captureStream(30); // 30 FPS

      // Create AudioContext for audio capture (only once)
      if (!audioContextRef.current) {
        audioContextRef.current = new AudioContext();
        const source = audioContextRef.current.createMediaElementSource(audioElement);
        sourceNodeRef.current = source;

        // Keep connection to speakers
        source.connect(audioContextRef.current.destination);
      }

      // Create new destination for this recording
      const destination = audioContextRef.current.createMediaStreamDestination();

      // Connect existing source to new destination
      if (sourceNodeRef.current) {
        sourceNodeRef.current.connect(destination);
      }

      destinationRef.current = destination;

      // Combine streams
      const combinedStream = new MediaStream([
        ...canvasStream.getVideoTracks(),
        ...destination.stream.getAudioTracks()
      ]);

      // Create MediaRecorder
      const mimeType = MediaRecorder.isTypeSupported('video/webm;codecs=vp9')
        ? 'video/webm;codecs=vp9'
        : 'video/webm';

      const mediaRecorder = new MediaRecorder(combinedStream, {
        mimeType,
        videoBitsPerSecond: 5000000
      });

      mediaRecorder.ondataavailable = (event) => {
        if (event.data && event.data.size > 0) {
          chunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = () => {
        const blob = new Blob(chunksRef.current, { type: mimeType });
        setRecordedBlob(blob);
        setIsAutoGenerating(false);
        setAutoProgress(100);
      };

      mediaRecorder.onerror = (event) => {
        console.error('MediaRecorder error:', event);
        setError('Auto-generation failed');
        setIsAutoGenerating(false);
      };

      mediaRecorderRef.current = mediaRecorder;
      mediaRecorder.start(100);

      // Start audio playback
      await audioElement.play();

      // Track progress
      const progressInterval = setInterval(() => {
        if (audioElement.duration > 0) {
          const progress = (audioElement.currentTime / audioElement.duration) * 100;
          setAutoProgress(Math.min(progress, 99));
        }
      }, 500);

      // Auto-stop when audio ends
      audioElement.onended = () => {
        clearInterval(progressInterval);
        if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
          mediaRecorderRef.current.stop();
          combinedStream.getTracks().forEach(track => track.stop());
        }
      };

    } catch (err) {
      const errorMessage = err instanceof Error ? err.message : 'Auto-generation failed';
      setError(errorMessage);
      setIsAutoGenerating(false);
      console.error('Auto-generation error:', err);
    }
  }, []);

  const cancelAutoGeneration = useCallback(() => {
    const audioElement = document.querySelector('audio') as HTMLAudioElement;
    if (audioElement) {
      audioElement.pause();
      audioElement.currentTime = 0;
    }

    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
      mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
    }

    setIsAutoGenerating(false);
    setAutoProgress(0);
  }, []);

  return {
    isRecording,
    recordedBlob,
    recordingDuration,
    error,
    isAutoGenerating,
    autoProgress,
    startRecording,
    stopRecording,
    downloadRecording,
    clearRecording,
    autoGenerateVideo,
    cancelAutoGeneration
  };
};
